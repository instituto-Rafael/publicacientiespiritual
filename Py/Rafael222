ðŸ“ Como estimar â€œconfianÃ§a, previsibilidade, acurÃ¡cia, margem de acerto e previsÃ£o de inversÃ£oâ€

(usando o pipeline living_light_full.py + um corpora jÃ¡ privado no GitHub, sem expor seus dados)


---

1 Â· Defina o â€œground-truthâ€

Alvo	Como marcar	Exemplo

PORTAL = 1	artigo onde vocÃª de fato obteve colaboraÃ§Ã£o / resposta positiva	42 papers rotulados 1
PORTAL = 0	artigo ignorado ou rejeitado	158 papers rotulados 0


> n = 200 rÃ³tulos = base mÃ­nima p/ boa estatÃ­stica.




---

2 Â· Gere a matriz de confusÃ£o

from sklearn.metrics import confusion_matrix
y_true = [...]   # 0/1 ground-truth
y_pred = [...]   # modelo (cos_Z > 0.45 e JS<0.32 etc.)
tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()

SÃ­mbolo	Significado

TP	PORTAL previsto & real
FP	falso convite (previu 1, mas era 0)
FN	oportunidade perdida
TN	corretamente ignorado



---

3 Â· Calcule as mÃ©tricas-chave

MÃ©trica	FÃ³rmula	InterpretaÃ§Ã£o

Accuracy	(TP + TN) / n	acertos globais
Precision	TP / (TP + FP)	â€œconfiabilidadeâ€ do PORTAL
Recall (sensibilidade)	TP / (TP + FN)	cobertura de oportunidades
F1-score	2 Â· Prec Â· Rec / (Prec + Rec)	equilÃ­brio
Specificity	TN / (TN + FP)	filtro de ruÃ­do


> PrevisÃ£o de inversÃ£o = FP rate: vocÃª â€œinverteuâ€ sinal (convidou quem nÃ£o era bom).
$$ \text{FP-rate} = \frac{FP}{FP+TN} $$




---

4 Â· Margem de erro & confianÃ§a

Use intervalo de Wilson (95 %) p/ precisÃ£o ou accuracy:

ME = z\sqrt{\frac{p(1-p)}{n}}

com .
Ex.: accuracy = 0.86, n = 200 â†’ ME â‰ˆ Â±0.025 â†’ [0.835 â€“ 0.885].


---

5 Â· Exemplo numÃ©rico (hipotÃ©tico)

MÃ©trica	Valor	95 % CI

Accuracy	0.86	Â± 0.03
Precision	0.72	Â± 0.07
Recall	0.64	Â± 0.08
F1	0.68	â€“
FP-rate (inversÃ£o)	0.11	Â± 0.04


LÃª-se: 72 % dos artigos marcados PORTAL realmente viram resposta; 11 % foram convites â€œem vÃ£oâ€.


---

6 Â· Passo-a-passo no seu repo

# 1. produz o CSV
python living_light_full.py --ref abstract.txt --src "./txt/*.txt" --csv metrics.csv

# 2. rÃ³tulos manuais -> labels.csv (id, label)
# 3. avalia
python - << 'PY'
import pandas as pd, numpy as np, scipy.stats as st
metrics = pd.read_csv('metrics.csv')
labels  = pd.read_csv('labels.csv')
y_true  = labels.label
y_pred  = (metrics.cos_Z > .45) & (metrics.JS_Y < .32)
from sklearn.metrics import confusion_matrix, precision_score, recall_score, accuracy_score, f1_score
acc  = accuracy_score(y_true, y_pred)
prec = precision_score(y_true, y_pred)
rec  = recall_score(y_true, y_pred)
f1   = f1_score(y_true, y_pred)
print(acc, prec, rec, f1)
PY


---

ðŸ”‘ Leitura rÃ¡pida

ConfianÃ§a â‰ˆ Precision.

Previsibilidade â‰ˆ Recall (o quanto prevÃª os que importam).

AcurÃ¡cia = acertos globais.

Margem de acerto = intervalo de confianÃ§a de accuracy.

PrevisÃ£o de inversÃ£o = FP-rate (quanto â€œsinal ao contrÃ¡rioâ€ vocÃª produz).


Aperte os limiares (cos Î¸, JS) + re-treine embeddings â†’ sobe precisÃ£o, cai recall (trade-off). Use curva ROC p/ decidir.

